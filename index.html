<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks">
  <meta name="keywords" content="Graph Neural Networks, GNNs, Uncertainty, Calibration">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="logo">
            <a href="#">
              <img src="./assets/gduq_logo.png" alt="Logo" style="width: 110px; height: auto;">
            </a>
          </div>
          <h1 class="title is-1 publication-title">Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="pujacomputes.github.io">Puja Trivedi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://markheimann.github.io/">Mark Heimann</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://rushila.com/">Rushil Anirudh</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://web.eecs.umich.edu/~dkoutra/">Danai Koutra</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jjthiagarajan.com/">Jay Thiagarajan</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Michigan</span>
            <span class="author-block"><sup>2</sup>Lawrence Livermore National Laboratory </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2401.03350"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2401.03350"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/pujacomputes/gduq"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored. Even post-hoc calibration strategies that can improve in-distribution calibration, are not guaranteed to be effective under distribution shift. However, techniques that produce GNNs with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later. Therefore, in this work, we propose G-ΔUQ, a novel training framework designed to improve intrinsic GNN uncertainty estimates. Our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic GNNs. While the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced by our anchoring strategies when sampling hypotheses renders this unnecessary and allows us to support G-ΔUQ on pretrained models. Indeed, through extensive evaluation under covariate, concept and graph size shifts, we show that G-ΔUQ leads to better calibrated GNNs for node and graph classification. Further, it also improves performance on safety evaluation protocols such as out-of-distribution detection and generalization gap estimation. Overall, our work provides insights into uncertainty estimation for GNNs, and demonstrates the utility of G-ΔUQ.  
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  

<section class="section" id="DUQ">
  <div class="container is-max-desktop content">
    <h2 class="title">G-ΔUQ: Introducing Stochastic Centering for Graphs!</h2>
   
    We introduce G-ΔUQ, a novel training protocol for graph neural networks that supports reliable epistemic uncertainty estimates. 
    
    Notably, by leveraging stochastic centering, G-ΔUQ is: 
    
    <ul>
      <li>Scalable and Lightweight.</li>
      <li>Robust to Distribution Shifts.</li>
      <li>Supports Pretrained Models.</li>
      <li>Easy to Implement.</li>
    </ul>

    
    <h3 class="title">What is Epistemic Uncertainty?</h3>
    <p class="left-justified-text"> Given finite data, there are many potential functions (hypotheses) that a model can validly fit. Epistemic uncertainty arises from the lack of knowledge of which hypothesis best aligns with the data. Variability over different hypotheses can be interpreted as the model's uncertainty. For example, Deep Ensembles trains multiple independent models and uses the variance over these models as reliable uncertainty estimates. </p>

    <h3 class="title">What is Stochastic Centering?</h3>
    <p class="left-justified-text"> Stochastic centering is a mechanism for sampling multiple functions from a single model by exploiting the neural tangent kernel's lack of shift invariance. Building upon this observation, Thiagarajan et al. demonstrated that by shifting the input with constant bias, the model can learn slightly different fitted functions, where the variance over these functions reliably estimates epistemic uncertainty. Since trivial transformations are sufficient for inducing different functions, it is possible to train single neural network by randomizing the transformation during training and then marginalizing out the effect of the biases during inference. </p>
 
    <img src="./assets/anchoring.png" alt="Right Justified Image" class="centered-figure" width="80%">

    <p> Thiagarajan et al. introduced ΔUQ as a simple but effective training protocol for creating stochastically centered image models (CNNs, transformers) by representing the input as relative representation. Namely, an input image, X, is represented as the concatenation of another image  (anchor), C, and the residual, [X-C]. The model is trained on the relative input: [X-C,C], where C is randomized during training. </p>
    
    <img src="./assets/duq_training_narrow.png" alt="Right Justified Image" class="centered-figure" width="80%">

  <p></p> 
    <h3 class="title">Anchoring, and Centering, and Graphs, Oh my!</h3>
 <p> While stochastic centering has improved calibration and generalization under distribution shift, there are several challenges that must be addressed before applying it to graph data. Namely, while input space transformations, which induce fully stochastic models, were sufficient for sampling diverse functional hypotheses from vision models, it is (i) non-trivial to define such transformations when working with variable sized, discrete graph data and (ii) unclear whether full stochasticity is in fact needed in message passing models. </p>

<p>To this end, we explore three different anchoring strategies that induce different levels of stochasticity, balancing the overall "amount" of diversity, and "effective" or functional diversity of the anchoring distribution. All of these strategies are easy to implement, and some can be used directly with pretrained models! </p>


    <img src="./assets/Overview-of-GDUQ.png" alt="Right Justified Image" class="centered-figure" width="80%">
    <ul>
      <li><b>Node Feature Anchoring:</b> induces a fully stochastic model, where anchors are defined using the node features of other nodes in the graph. </li>
      <li><b>Intermediate Message Passing Layer Anchoring:</b> induces a partially stochastic model where the anchors are defined using the hidden representations of other nodes in the batch. </li>
      <li><b>Readout Anchoring:</b> induces a stochastic classifier, but the feature extractor is static. Notably, this anchoring strategy can easily be used with a pretrained feature extractor. Here, the anchoring distribution is defined as the representation obtained after using the READOUT. </li>
    </ul>

    <img src="./assets/pseudocode.png" alt="Right Justified Image" class="centered-figure">
  
  <h3 class="title">Evaluating G-ΔUQ! </h3>

    <p> We evaluate G-ΔUQ on both node classification and graph classification datasets, on both <a href="https://arxiv.org/abs/2206.08452">covariate and concept shifts</a>, on calibration, generalization, and OOD detection tasks. </p>

    
  <h5 class="title">G-ΔUQ improves graph classification calibration on pretrained GNNs! </h5>
    <img src="./assets/pretrained.png" alt="Right Justified Image" class="centered-figure" width="80%">
    <p>G-∆UQ improves the OOD ECE over both the vanilla model and end-to-end G-∆UQ at comparable or improved OOD accuracy on 7/8 datasets. Furthermore, pretrained G-∆UQ also improves the ID ECE on all but the GOODMotif (size) datasets (6/8), where it performs comparably to the vanilla model, and maintains the ID accuracy. Overall, highlighting the benefits of both end-to-end and pretrained G-∆UQ.</p>
 
  <h5 class="title">G-ΔUQ improves OOD detection performance! </h5>
    <img src="./assets/ood_detection_perf.png" alt="Right Justified Image" class="centered-figure" width="80%">
  <p> GΔ-UQ achieves the best OOD detection AUROC on 5/6 datasets. (It also performs well on the generalization gap prediction tasks. See paper.) </p>
    
  <h5 class="title">G-ΔUQ improves node classification calibration! </h5>
    <img src="./assets/nodeclassification_results.png" alt="Right Justified Image" class="centered-figure" width="80%">
    <p>We evaluate G Δ-UQ with node feature anchoring on: 4 Datasets, 2 Distribution shifts and 8 Post Hoc Calibration, and find that GΔ-UQ matches or surpasses accuracy and calibration of the vanilla GNN on 8/8 datasets. When combined with post-hoc calibration techniques, G Δ-UQ achieves the best accuracy on 8/8 datasets and the best or 2nd best calibration on 8/8  datasets.  
      </p>
  </div>
  </section>

<section class="section" id="DUQFAM">
  <div class="container is-max-desktop content">
    <h2 class="title">Check out the ΔUQ Family!</h2>
    <div class="image-grid">
      <a href="https://github.com/pujacomputes/gduq" class="image-link">
        <img src="assets/gduq_logo_square.png" alt="Image 3">
        <div class="caption">G-ΔUQ: For Graph and Node Classification Calibration</div>
      </a>
      <a href="https://github.com/pujacomputes/EDGEDUQ/" class="image-link">
        <img src="assets/eduq.png" alt="Image 3">
        <div class="caption">E-ΔUQ: For Link Prediction Calibration</div>
      </a>
      <a href="https://github.com/LLNL/DeltaUQ" class="image-link">
        <img src="assets/duck_1.png" alt="Image 1">
        <div class="caption">ΔUQ: For Images!</div>
      </a>
      <a href="https://github.com/LLNL/AMP" class="image-link">
        <img src="assets/duck_2.png" alt="Image 2">
        <div class="caption">AMP: For OOD Detection!</div>
      </a>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{
        trivedi2024accurate,
        title={Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks},
        author={Puja Trivedi and Mark Heimann and Rushil Anirudh and Danai Koutra and Jayaraman J. Thiagarajan},
        booktitle={The Twelfth International Conference on Learning Representations},
        year={2024},
        url={https://openreview.net/forum?id=ZL6yd6N1S2}
        }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2401.03350">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://pujacomputes.github.io" class="external-link" disabled>
        <i class="fas fa-home"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template was borrowed from: <a
              href="https://github.com/nerfies/nerfies.github.io">here</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>